{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg as la\n",
    "import sklearn.metrics as met\n",
    "import scipy.linalg as scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we add our 1 term so the bias can be included in our weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read MNIST training data\n",
    "df = pd.read_csv('data/mnist_train.csv')\n",
    "df.insert(1, 'bias', 1.0)\n",
    "\n",
    "\n",
    "X = df.iloc[:, 1:].to_numpy()       # values are scaled to be between 0 and 1\n",
    "X[:,2:] /= 255.0\n",
    "Y = df.iloc[:, 0].to_numpy()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we filter out the columns that are 0 for more than 90% of the entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 344)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage of non-zero values in each column\n",
    "column_percentages = np.mean(X != 0, axis=0)\n",
    "# Identify columns where the percentage is greater than 10% (90% are 0)\n",
    "selected_columns = column_percentages > 0.1\n",
    "# Create a new matrix with the selected columns\n",
    "cleaned_train = X[:, selected_columns]\n",
    "print(cleaned_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the label vectors for each one of our classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_train = list()\n",
    "\n",
    "for digit in df['label'].unique():\n",
    "    label_train = np.array([1 if item == digit else -1 for item in Y])\n",
    "    list_train.append(label_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we map which vector classifier has the highest confidence to its associated label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping function to map numbers to strings\n",
    "def map_to_digit(number):\n",
    "    mapping = {# 5 0 4 1 9 2 3 6 7 8\n",
    "        0: 5,\n",
    "        1: 0,\n",
    "        2: 4,\n",
    "        3: 1,\n",
    "        4: 9,\n",
    "        5: 2,\n",
    "        6: 3,\n",
    "        7: 6,\n",
    "        8: 7,\n",
    "        9: 8,\n",
    "\n",
    "    }\n",
    "    return mapping.get(number, -1)\n",
    "\n",
    "def get_classification(data,weights):\n",
    "    guesses = np.argmax(data@(np.vstack(weights).T),axis=1)\n",
    "    digits = np.vectorize(map_to_digit)(guesses)\n",
    "    return digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the step where we preform QR decomposition and obtain the minimized weights for each one of our classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QR composition\n",
    "Q, R = np.linalg.qr(cleaned_train)\n",
    "weight_list = list()\n",
    "for label in list_train:\n",
    "    weight_list.append(scipy.solve_triangular(R,(Q.T@label))) #returns our weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we obtain our classifications and compute the confusion matrix. Our error rate here is 15% which seems to classify the training data well, now let us see the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted     0     1     2     3     4     5     6     7     8     9    All\n",
      "Actual                                                                      \n",
      "0          5578     6    32    22    21    88   104    10    58     4   5923\n",
      "1             2  6536    44    16     6    38     6    21    66     7   6742\n",
      "2            82   301  4729   131   132    16   168   171   195    33   5958\n",
      "3            36   181   193  5049    24   121    82   167   140   138   6131\n",
      "4             8   108    50     3  5088    34    98    42    75   336   5842\n",
      "5           190   112    34   498   131  3718   260    95   242   141   5421\n",
      "6            85    88    50     5    55    89  5518     2    23     3   5918\n",
      "7            64   190    49    44   129     9    10  5536    17   217   6265\n",
      "8            57   489    64   227    88   155   103    49  4446   173   5851\n",
      "9            78    83    36   107   374    29    16   396    46  4784   5949\n",
      "All        6180  8094  5281  6102  6048  4297  6365  6489  5308  5836  60000\n",
      "Error rate is  0.1503\n"
     ]
    }
   ],
   "source": [
    "training_classifications = get_classification(cleaned_train,weight_list)\n",
    "df_confusion = pd.crosstab(Y, training_classifications, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "print(df_confusion)\n",
    "\n",
    "\n",
    "print(\"Error rate is \", np.sum(Y != training_classifications)/Y.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the same steps for our testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('data/mnist_test.csv')\n",
    "df2.insert(1, 'bias', 1.0)\n",
    "\n",
    "\n",
    "X_test = df2.iloc[:, 1:].to_numpy()       # values are scaled to be between 0 and 1\n",
    "X_test[:,2:] /= 255.0\n",
    "Y_test = df2.iloc[:, 0].to_numpy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 344)\n"
     ]
    }
   ],
   "source": [
    "cleaned_test = X_test[:, selected_columns]\n",
    "print(cleaned_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we observed an accuracy of 14.4% percent. This surprising as it performed better than the training data, but it is still roughly similar. This indicates our model did not overfit which is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted     0     1    2     3     4    5     6     7    8    9    All\n",
      "Actual                                                                  \n",
      "0           936     1    3     2     1   12    17     2    5    1    980\n",
      "1             0  1099    3     3     2    1     5     3   19    0   1135\n",
      "2            18    59  822    19    16    1    23    34   36    4   1032\n",
      "3             7    21   26   863     5   16    12    32   18   10   1010\n",
      "4             1    24    5     1   864    3    16     3   10   55    982\n",
      "5            23    16    4    82    27  610    38    31   42   19    892\n",
      "6            15    10    5     0    19   14   892     1    2    0    958\n",
      "7             4    42   15     6    16    0     3   899    0   43   1028\n",
      "8            14    50   10    34    19   22    26    18  757   24    974\n",
      "9            19    13    7    12    70    5     4    57    7  815   1009\n",
      "All        1037  1335  900  1022  1039  684  1036  1080  896  971  10000\n",
      "Error rate is  0.1443\n"
     ]
    }
   ],
   "source": [
    "tst_class = get_classification(cleaned_test,weight_list)\n",
    "df_confusion2 = pd.crosstab(Y_test, tst_class, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "print(df_confusion2)\n",
    "\n",
    "\n",
    "print(\"Error rate is \", np.sum(Y_test != tst_class)/Y_test.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.41738431e-01 -4.47561204e-02  1.57613190e-02  2.92590027e-02\n",
      " -3.93377988e-02  2.77487481e-02 -3.11306992e-02 -1.22409507e-02\n",
      " -4.54178206e-02 -3.01630712e-03 -5.98146399e-02 -8.23959563e-02\n",
      " -8.06097991e-03  6.46713148e-04 -2.24318270e-02  3.13106751e-02\n",
      "  5.94932211e-04  1.61432524e-02  1.17897664e-02  1.56509766e-02\n",
      " -4.17891929e-05  2.28552504e-02  7.97734181e-03 -6.74672232e-02\n",
      " -1.40812316e-02  2.00630290e-02  1.13748734e-03 -1.51752608e-02\n",
      " -7.14412335e-04  7.07339537e-03  2.94859972e-02  1.73632499e-02\n",
      "  6.00248178e-03  2.47610234e-02  1.07122495e-02  2.44609048e-02\n",
      "  1.53369213e-03  4.29942195e-03  5.32495106e-02 -7.90213807e-02\n",
      "  1.85818510e-02 -2.25485636e-02  3.51359259e-02 -1.45260412e-02\n",
      "  1.82861033e-02  1.92334327e-02  1.80033481e-02  3.42737646e-02\n",
      "  2.98748796e-02  1.41713595e-03  3.60368860e-02  3.04466877e-02\n",
      "  1.18512690e-02 -1.76865316e-02 -3.40373433e-02  9.53718320e-02\n",
      " -1.85399712e-01  1.73573948e-02 -7.00988063e-03  5.09844727e-03\n",
      "  4.38805957e-03  2.19388723e-03 -1.04409781e-02  1.48003538e-02\n",
      "  4.90143129e-03  1.70381540e-02  1.55797387e-02  5.18678682e-02\n",
      "  1.78521421e-02  1.37501237e-02  6.18616967e-03  2.74142678e-02\n",
      " -2.72502502e-02  2.61155861e-02 -6.01589869e-02 -2.84194762e-02\n",
      "  2.23548534e-02  7.54425588e-03 -3.06442949e-03  7.96899721e-03\n",
      " -2.12618230e-02 -4.77408556e-03  2.49348749e-02  1.98493154e-02\n",
      "  2.99847146e-02  5.31991697e-03  1.16959080e-02  6.06955952e-03\n",
      "  3.91412634e-02  1.02935624e-02  3.03929617e-03 -3.21075890e-02\n",
      "  2.72696477e-02 -2.81171622e-02 -7.14170274e-03 -2.32364450e-02\n",
      "  2.68782451e-02 -2.26638343e-03  1.22595822e-02  1.22410362e-02\n",
      "  1.00285907e-02  1.04416481e-02  2.92563453e-02  1.45082207e-02\n",
      "  2.86456706e-02  2.99930531e-02  1.55762746e-03  5.74063755e-02\n",
      "  1.26364118e-02  6.42806333e-02 -4.32180759e-02  3.45231533e-02\n",
      "  7.70277135e-03 -1.27322438e-02  1.82667544e-02  1.60492629e-02\n",
      " -2.08820447e-02 -1.89166559e-03 -4.02502706e-02 -2.13706118e-02\n",
      " -2.26036941e-02  6.07322455e-03  1.04222455e-02  4.51770489e-02\n",
      "  3.44649005e-02  3.47499268e-02  1.08129176e-01  1.16698734e-02\n",
      " -1.12691784e-02  1.99551782e-02  1.22909762e-02  2.11747264e-02\n",
      " -2.14839081e-02  9.16121392e-03 -1.15262421e-02 -7.01637631e-02\n",
      " -2.58715963e-02 -3.59245406e-02 -2.92140285e-02 -3.69969923e-02\n",
      " -2.35930453e-02  2.42215183e-02 -2.91636812e-02  1.11592333e-01\n",
      " -2.71511848e-03  1.54966031e-02 -1.45599823e-02  1.77372994e-02\n",
      "  1.11958732e-02  6.97918576e-03 -3.14154260e-02 -4.44545420e-02\n",
      " -4.55726674e-02 -2.67002223e-02 -2.60898565e-02 -5.00006092e-02\n",
      " -5.62607184e-02 -3.59176031e-02 -3.79117383e-02 -2.17395620e-02\n",
      "  1.67170250e-01  5.90165526e-02  2.28892034e-02  1.71108048e-02\n",
      "  1.79932479e-02  5.35107192e-02 -5.64117898e-03 -5.24348640e-02\n",
      "  1.19048092e-02 -6.69314545e-02  7.39346722e-03 -4.36443888e-02\n",
      " -4.40170068e-02 -3.16471407e-02 -4.46221135e-02  2.34592090e-02\n",
      "  6.28961342e-02  3.36180554e-02  6.08492505e-02 -1.41887384e-02\n",
      "  3.60946257e-02  3.55332258e-02  4.02607568e-02  3.41744785e-02\n",
      " -2.21091595e-02 -2.73236150e-02 -2.30007955e-02 -5.60525036e-02\n",
      " -6.61774875e-03 -5.31379136e-02 -4.25685557e-02 -3.23122574e-02\n",
      " -1.54575792e-02  3.74962882e-02 -1.76311625e-02  1.92146403e-02\n",
      "  1.90146605e-02  2.83333682e-02 -8.24660507e-03  3.91303649e-02\n",
      "  6.62773343e-03  2.48068647e-02 -6.91077716e-03 -5.95561719e-02\n",
      " -3.53820419e-02 -3.09532260e-02 -3.27274556e-02 -4.88735917e-02\n",
      " -4.61060560e-02 -1.88639569e-03  9.00081206e-03  1.81910757e-02\n",
      "  1.80733513e-03 -2.82236390e-02  3.58659199e-02  4.63304509e-02\n",
      "  2.18400559e-02  1.53033247e-03  1.25334789e-02  2.46967576e-02\n",
      " -1.49580014e-02 -1.18717050e-02 -4.85167415e-02 -5.39557555e-02\n",
      " -4.76142363e-02 -1.73863392e-02  1.39491301e-02 -2.38143623e-03\n",
      "  2.16290397e-02 -1.40759427e-02 -2.03242421e-02  6.07945900e-02\n",
      "  4.60317370e-02  9.42800957e-03  4.09747116e-02  4.09183365e-02\n",
      " -3.11330240e-03 -6.90316974e-03  9.07730228e-03 -2.51412975e-02\n",
      " -1.29698464e-02 -3.53216583e-02  6.09946561e-04 -9.91501310e-03\n",
      " -8.69128318e-03 -1.77282770e-02 -6.61022451e-03 -2.82513706e-02\n",
      " -2.78032349e-03 -2.92797958e-02 -2.70491965e-02  2.12091714e-02\n",
      "  7.56586484e-03  1.34045107e-02  3.35878375e-02 -1.11132034e-02\n",
      "  2.07208158e-02  5.26340143e-03  1.26523220e-02  3.31762988e-03\n",
      " -1.41316882e-02 -5.14671363e-03 -7.55976090e-03  1.69895784e-03\n",
      " -1.54870284e-02 -2.89015365e-03  3.12525294e-03 -4.97573072e-02\n",
      "  1.38041037e-02  1.10062241e-03 -2.00981722e-02  9.41910324e-03\n",
      "  1.91727265e-03  1.89570499e-02  6.07507813e-04  2.56509271e-02\n",
      "  1.35816022e-02  3.44250119e-03  6.83737998e-03 -3.22832212e-02\n",
      "  1.17191832e-02 -2.47806857e-02 -2.72107422e-04 -2.47682916e-02\n",
      "  2.29604485e-02 -5.29201752e-02  3.70879043e-03 -1.91359545e-02\n",
      " -4.83801484e-03  2.43538371e-02  1.45654748e-02  1.67313248e-02\n",
      " -1.13540757e-02 -2.14324519e-02 -1.02232057e-04  2.17694921e-02\n",
      "  3.26323331e-03 -3.16752676e-02  4.65778609e-03 -1.23181854e-02\n",
      "  3.79648266e-03 -4.99727736e-02  3.04295261e-02  1.89091882e-02\n",
      "  4.39107266e-03  1.67664057e-02  5.10631508e-03  2.57812187e-02\n",
      "  2.58799721e-02  5.69470660e-02  2.28693802e-02 -7.59142232e-03\n",
      "  2.10343805e-03 -5.60123138e-03 -3.65212359e-02  1.14745029e-02\n",
      " -5.91011022e-02 -1.13437873e-02  1.77290494e-02  1.80769209e-02\n",
      "  5.06639416e-03  2.06878584e-02  3.37766026e-02  3.77139772e-03\n",
      "  3.02687340e-02  3.89901081e-02  1.86734114e-02  2.73046246e-02\n",
      "  2.54971317e-03 -7.25298964e-03 -9.21254694e-02 -4.28863781e-02\n",
      " -4.55271918e-02 -3.21400360e-02 -2.34379837e-02 -3.14809449e-02\n",
      " -2.61258269e-02 -1.41551136e-02 -2.52524878e-02 -3.65754113e-03\n",
      " -1.05734341e-01 -2.89782030e-02 -2.99946084e-02 -7.84518854e-02]\n"
     ]
    }
   ],
   "source": [
    "#print(weight_list[1])\n",
    "newY = np.array([1 if item == 0 else -1 for item in Y_test])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mathML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
