{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start this using the same steps as Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg as la\n",
    "import sklearn.metrics as met\n",
    "import scipy.linalg as scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read MNIST training data\n",
    "df = pd.read_csv('data/mnist_train.csv')\n",
    "df.insert(1, 'bias', 1.0)\n",
    "\n",
    "\n",
    "X = df.iloc[:, 1:].to_numpy()       # values are scaled to be between 0 and 1\n",
    "X[:,2:] /= 255.0\n",
    "Y = df.iloc[:, 0].to_numpy()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 344)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage of non-zero values in each column\n",
    "column_percentages = np.mean(X != 0, axis=0)\n",
    "# Identify columns where the percentage is greater than 10% (90% are 0)\n",
    "selected_columns = column_percentages > 0.1\n",
    "# Create a new matrix with the selected columns\n",
    "X= X[:, selected_columns]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we generate a random matrix and multiply it by X transpose, then we use the maximum function to capture positive correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rand= np.random.choice([-1, 1], size=(5000, 344))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.43529412 11.84313725  5.97647059 ... -3.30588235  9.85098039\n",
      "  12.33333333]\n",
      " [11.77254902  9.4745098   9.34901961 ...  3.79215686 17.01960784\n",
      "   6.75686275]\n",
      " [-0.97254902 -1.12941176 -6.02352941 ...  5.38431373 -1.5372549\n",
      "  -2.76470588]\n",
      " ...\n",
      " [ 0.38431373 20.18039216  9.70196078 ... -5.6745098   4.26666667\n",
      "  17.95686275]\n",
      " [ 4.29019608 -8.61176471  8.87843137 ...  3.16470588 -7.43529412\n",
      "   1.35294118]\n",
      " [12.76862745  9.63921569 -2.41568627 ...  2.09019608  7.5372549\n",
      "  24.09803922]]\n"
     ]
    }
   ],
   "source": [
    "RX = Rand@X.T\n",
    "print(RX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         11.84313725  5.97647059 ...  0.          9.85098039\n",
      "  12.33333333]\n",
      " [11.77254902  9.4745098   9.34901961 ...  3.79215686 17.01960784\n",
      "   6.75686275]\n",
      " [ 0.          0.          0.         ...  5.38431373  0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.38431373 20.18039216  9.70196078 ...  0.          4.26666667\n",
      "  17.95686275]\n",
      " [ 4.29019608  0.          8.87843137 ...  3.16470588  0.\n",
      "   1.35294118]\n",
      " [12.76862745  9.63921569  0.         ...  2.09019608  7.5372549\n",
      "  24.09803922]]\n"
     ]
    }
   ],
   "source": [
    "RX = np.maximum(0,RX)\n",
    "print(RX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concatenate this matrix to our feature matrix and then use a binary classifier like the ones in Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_matrix = np.concatenate((X,RX.T),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = np.array([1 if item ==0 else -1 for item in Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, R = scipy.qr(result_matrix,mode='economic',check_finite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = scipy.solve_triangular(R,(Q.T@label_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5344,)\n"
     ]
    }
   ],
   "source": [
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = result_matrix@weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "[-1.02357496  1.12024667 -1.13545865 ... -0.98007273 -0.92337733\n",
      " -0.98308799]\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = np.array([1 if item > 0.0 else -1 for item in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1 -1 ... -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "print(classifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe our error rate to be 0.2% this is the lowest error rate so far computed for any of the binary classifiers written for our MNIST questions.It seems these features capture some latent information in the image that gives more information than standard pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted     -1     1    All\n",
      "Actual                       \n",
      "-1         54051    26  54077\n",
      "1            111  5812   5923\n",
      "All        54162  5838  60000\n",
      "Error rate is  0.0022833333333333334\n"
     ]
    }
   ],
   "source": [
    "df_confusion = pd.crosstab(label_train, classifications, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "print(df_confusion)\n",
    "\n",
    "\n",
    "print(\"Error rate is \", np.sum(label_train != classifications)/label_train.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read MNIST training data\n",
    "df = pd.read_csv('data/mnist_test.csv')\n",
    "df.insert(1, 'bias', 1.0)\n",
    "\n",
    "\n",
    "X = df.iloc[:, 1:].to_numpy()       # values are scaled to be between 0 and 1\n",
    "X[:,2:] /= 255.0\n",
    "Y = df.iloc[:, 0].to_numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 344)\n"
     ]
    }
   ],
   "source": [
    "X= X[:, selected_columns]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.43921569 -8.68235294  1.24705882 ...  2.21960784  3.34117647\n",
      "   9.36078431]\n",
      " [-0.03921569 10.10980392  6.45490196 ... 14.47843137 11.6627451\n",
      "  29.34509804]\n",
      " [ 9.49019608 -7.96078431 -7.29411765 ...  3.45882353 -3.29411765\n",
      "   2.00392157]\n",
      " ...\n",
      " [ 8.79215686  1.45882353 -2.24313725 ... 12.84705882 11.1372549\n",
      "   6.14509804]\n",
      " [-2.6745098  -1.75686275  8.03921569 ... 11.76470588  2.\n",
      "  -6.60784314]\n",
      " [ 3.19215686  3.9372549   9.70980392 ...  9.31764706  8.94117647\n",
      "  31.37647059]]\n"
     ]
    }
   ],
   "source": [
    "RX = Rand@X.T\n",
    "print(RX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.43921569  0.          1.24705882 ...  2.21960784  3.34117647\n",
      "   9.36078431]\n",
      " [ 0.         10.10980392  6.45490196 ... 14.47843137 11.6627451\n",
      "  29.34509804]\n",
      " [ 9.49019608  0.          0.         ...  3.45882353  0.\n",
      "   2.00392157]\n",
      " ...\n",
      " [ 8.79215686  1.45882353  0.         ... 12.84705882 11.1372549\n",
      "   6.14509804]\n",
      " [ 0.          0.          8.03921569 ... 11.76470588  2.\n",
      "   0.        ]\n",
      " [ 3.19215686  3.9372549   9.70980392 ...  9.31764706  8.94117647\n",
      "  31.37647059]]\n"
     ]
    }
   ],
   "source": [
    "RX = np.maximum(0,RX)\n",
    "result_matrix = np.concatenate((X,RX.T),axis=1)\n",
    "print(RX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test = np.array([1 if item ==0 else -1 for item in Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = result_matrix@weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "[-0.94851943 -0.79179232 -1.05804777 ... -0.91271914 -1.0789264\n",
      " -0.98047174]\n",
      "[-1 -1 -1 ... -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "print(predictions_test.shape)\n",
    "print(predictions_test)\n",
    "classifications_test = np.array([1 if item > 0.0 else -1 for item in predictions_test])\n",
    "print(classifications_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe the test error rate to be almost exactly like our training data. This means that our model is generalizing and these features we generated carry more information about unseen data as well. This classifier did way better than our Q5 binary classifier for 0. Our binary classifier had an error rate 0.0182 (screenshot provided in submission). This is roughly 7x increase in accuracy for our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted    -1    1    All\n",
      "Actual                     \n",
      "-1         9014    6   9020\n",
      "1            19  961    980\n",
      "All        9033  967  10000\n",
      "Error rate is  0.0025\n"
     ]
    }
   ],
   "source": [
    "df_confusion = pd.crosstab(label_test, classifications_test, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "print(df_confusion)\n",
    "\n",
    "\n",
    "print(\"Error rate is \", np.sum(label_test != classifications_test)/label_test.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
